{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e58dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.callbacks.mlflow_callback import MlflowCallbackHandler\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3050a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/utils/autologging_utils/__init__.py:472: FutureWarning: \u001b[31mThe `log_models` parameter's behavior will be changed in a future release. MLflow no longer logs model artifacts automatically, use `mlflow.openai.log_model` to log model artifacts manually if needed.\u001b[0m\n",
      "  return _autolog(*args, **kwargs)\n",
      "2025/08/16 18:58:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for openai.\n"
     ]
    }
   ],
   "source": [
    "# if using sklearn models\n",
    "mlflow.autolog()\n",
    "# Set experiment name (this will create it if it doesn't exist)\n",
    "mlflow.set_experiment(\"New Experiment Langgraph\")\n",
    "# If using OpenAI models, you can enable autologging for OpenAI\n",
    "mlflow.openai.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b315d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run `python -m spacy download en_core_web_sm` to download en_core_web_sm model for text visualization.\n"
     ]
    }
   ],
   "source": [
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "api_key_ = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    callbacks=[MlflowCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8826ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025/08/16 18:59:25 WARNING mlflow.openai._openai_autolog: Encountered unexpected error when ending trace: 'NotGiven' object is not iterable\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/openai/_openai_autolog.py\", line 352, in _end_span_on_success\n",
      "    set_span_chat_attributes(span, inputs, result)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/openai/utils/chat_schema.py\", line 46, in set_span_chat_attributes\n",
      "    if tools := _parse_tools(inputs):\n",
      "                ~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/openai/utils/chat_schema.py\", line 261, in _parse_tools\n",
      "    for tool in tools:\n",
      "                ^^^^^\n",
      "TypeError: 'NotGiven' object is not iterable\n",
      "2025/08/16 18:59:26 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/langchain/_langchain_autolog.py:241: UserWarning: MLflow autologging does not support logging models containing BaseRetriever because logging the model requires `loader_fn` and `persist_dir`. Please log the model manually using `mlflow.langchain.log_model(model, artifact_path, loader_fn=..., persist_dir=...)`\"\n",
      "2025/08/16 18:59:26 WARNING mlflow.langchain._langchain_autolog: Failed to log model due to error MLflow langchain flavor only supports subclasses of (<class 'langchain.chains.base.Chain'>, <class 'langchain.agents.agent.AgentExecutor'>, <class 'langchain_core.retrievers.BaseRetriever'>, <class 'langchain_core.language_models.chat_models.SimpleChatModel'>, <class 'langchain_core.prompts.chat.ChatPromptTemplate'>, <class 'langchain_core.runnables.passthrough.RunnablePassthrough'>, <class 'langchain_core.runnables.base.RunnableLambda'>, <class 'langchain_core.runnables.base.RunnableParallel'>, <class 'langchain_core.runnables.base.RunnableSequence'>, <class 'langchain_core.runnables.branch.RunnableBranch'>, <class 'langchain_core.runnables.passthrough.RunnableAssign'>, <class 'langchain_core.runnables.base.RunnableBinding'>, <class 'langgraph.graph.graph.CompiledGraph'>), found CompiledStateGraph..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WeatherResponse(conditions='sunny')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Callable, List, Optional\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    conditions: str = Field(\n",
    "        description=\"The weather conditions in the specified city.\",\n",
    "        example=\"rainy\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    response_format=WeatherResponse  \n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "response[\"structured_response\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
