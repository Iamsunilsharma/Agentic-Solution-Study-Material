{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e58dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.callbacks.mlflow_callback import MlflowCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e58f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress MLflow autologging warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"mlflow\")\n",
    "\n",
    "# Target just the mlflow.langchain logger\n",
    "logging.getLogger(\"mlflow.langchain._langchain_autolog\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3050a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/16 19:19:07 INFO mlflow.tracking.fluent: Autologging successfully enabled for openai.\n"
     ]
    }
   ],
   "source": [
    "# if using sklearn models\n",
    "mlflow.autolog()\n",
    "# Set experiment name (this will create it if it doesn't exist)\n",
    "mlflow.set_experiment(\"New Experiment Langgraph\")\n",
    "# If using OpenAI models, you can enable autologging for OpenAI\n",
    "mlflow.openai.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b315d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run `python -m spacy download en_core_web_sm` to download en_core_web_sm model for text visualization.\n"
     ]
    }
   ],
   "source": [
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "api_key_ = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    callbacks=[MlflowCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8826ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/16 19:19:10 INFO mlflow.tracking.fluent: Autologging successfully enabled for langchain.\n",
      "2025/08/16 19:19:10 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Span for run_id 2d55c6b0-a289-4463-bb1b-39f7b27a892e not found.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025/08/16 19:19:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Span for run_id b931680b-b3b6-45a3-bd0e-a0fcc0e7efe7 not found.\n",
      "2025/08/16 19:19:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Span for run_id dc176f69-2a36-4429-9620-7bdbc6f3ea77 not found.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025/08/16 19:19:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Span for run_id 3c023976-85b7-4170-a85a-61b380e1ad12 not found.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025/08/16 19:19:13 WARNING mlflow.openai._openai_autolog: Encountered unexpected error when ending trace: 'NotGiven' object is not iterable\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/openai/_openai_autolog.py\", line 352, in _end_span_on_success\n",
      "    set_span_chat_attributes(span, inputs, result)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/openai/utils/chat_schema.py\", line 46, in set_span_chat_attributes\n",
      "    if tools := _parse_tools(inputs):\n",
      "                ~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/Users/sunilsharma/.pyenv/versions/genai/lib/python3.13/site-packages/mlflow/openai/utils/chat_schema.py\", line 261, in _parse_tools\n",
      "    for tool in tools:\n",
      "                ^^^^^\n",
      "TypeError: 'NotGiven' object is not iterable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WeatherResponse(conditions='sunny')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeatherResponse(BaseModel):\n",
    "    conditions: str = Field(\n",
    "        description=\"The weather conditions in the specified city.\",\n",
    "        example=\"rainy\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    response_format=WeatherResponse  \n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "response[\"structured_response\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
